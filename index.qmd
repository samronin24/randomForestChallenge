---
title: "Random Forest Challenge"
subtitle: "The Power of Weak Learners"
format:
  html: default
execute:
  echo: false
  eval: true
---

# ðŸŒ² Random Forest Challenge - The Power of Weak Learners

## The Problem: Can Many Weak Learners Beat One Strong Learner?

**Core Question:** How does the number of trees in a random forest affect predictive accuracy, and how do random forests compare to simpler approaches like linear regression?

**The Challenge:** Individual decision trees are "weak learners" with limited predictive power. Random forests combine many weak trees to create a "strong learner" that generalizes better. But how many trees do we need? Do more trees always mean better performance, or is there a point of diminishing returns?

**Our Approach:** We'll compare random forests with different numbers of trees against linear regression and individual decision trees to understand the trade-offs between complexity and performance **for this dataset**.

### Python

```{python}
#| label: performance-comparison-python
#| echo: false
#| fig-width: 10
#| fig-height: 6

# Import necessary libraries
import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.datasets import make_regression

# Create sample dataset
X, y = make_regression(n_samples=1000, n_features=10, noise=0.1, random_state=42)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create random forest models with different numbers of trees
rf_1 = RandomForestRegressor(n_estimators=1, random_state=42)
rf_5 = RandomForestRegressor(n_estimators=5, random_state=42)
rf_25 = RandomForestRegressor(n_estimators=25, random_state=42)
rf_100 = RandomForestRegressor(n_estimators=100, random_state=42)
rf_500 = RandomForestRegressor(n_estimators=500, random_state=42)
rf_1000 = RandomForestRegressor(n_estimators=1000, random_state=42)
rf_2000 = RandomForestRegressor(n_estimators=2000, random_state=42)
rf_5000 = RandomForestRegressor(n_estimators=5000, random_state=42)

# Fit the models
rf_1.fit(X_train, y_train)
rf_5.fit(X_train, y_train)
rf_25.fit(X_train, y_train)
rf_100.fit(X_train, y_train)
rf_500.fit(X_train, y_train)
rf_1000.fit(X_train, y_train)
rf_2000.fit(X_train, y_train)
rf_5000.fit(X_train, y_train)

# Calculate predictions for test data
predictions_1_test = rf_1.predict(X_test)
predictions_5_test = rf_5.predict(X_test)
predictions_25_test = rf_25.predict(X_test)
predictions_100_test = rf_100.predict(X_test)
predictions_500_test = rf_500.predict(X_test)
predictions_1000_test = rf_1000.predict(X_test)
predictions_2000_test = rf_2000.predict(X_test)
predictions_5000_test = rf_5000.predict(X_test)

# Calculate predictions for training data
predictions_1_train = rf_1.predict(X_train)
predictions_5_train = rf_5.predict(X_train)
predictions_25_train = rf_25.predict(X_train)
predictions_100_train = rf_100.predict(X_train)
predictions_500_train = rf_500.predict(X_train)
predictions_1000_train = rf_1000.predict(X_train)
predictions_2000_train = rf_2000.predict(X_train)
predictions_5000_train = rf_5000.predict(X_train)

# Calculate performance metrics for test data
rmse_1_test = np.sqrt(mean_squared_error(y_test, predictions_1_test))
rmse_5_test = np.sqrt(mean_squared_error(y_test, predictions_5_test))
rmse_25_test = np.sqrt(mean_squared_error(y_test, predictions_25_test))
rmse_100_test = np.sqrt(mean_squared_error(y_test, predictions_100_test))
rmse_500_test = np.sqrt(mean_squared_error(y_test, predictions_500_test))
rmse_1000_test = np.sqrt(mean_squared_error(y_test, predictions_1000_test))
rmse_2000_test = np.sqrt(mean_squared_error(y_test, predictions_2000_test))
rmse_5000_test = np.sqrt(mean_squared_error(y_test, predictions_5000_test))

# Calculate performance metrics for training data
rmse_1_train = np.sqrt(mean_squared_error(y_train, predictions_1_train))
rmse_5_train = np.sqrt(mean_squared_error(y_train, predictions_5_train))
rmse_25_train = np.sqrt(mean_squared_error(y_train, predictions_25_train))
rmse_100_train = np.sqrt(mean_squared_error(y_train, predictions_100_train))
rmse_500_train = np.sqrt(mean_squared_error(y_train, predictions_500_train))
rmse_1000_train = np.sqrt(mean_squared_error(y_train, predictions_1000_train))
rmse_2000_train = np.sqrt(mean_squared_error(y_train, predictions_2000_train))
rmse_5000_train = np.sqrt(mean_squared_error(y_train, predictions_5000_train))

r2_1 = r2_score(y_test, predictions_1_test)
r2_5 = r2_score(y_test, predictions_5_test)
r2_25 = r2_score(y_test, predictions_25_test)
r2_100 = r2_score(y_test, predictions_100_test)
r2_500 = r2_score(y_test, predictions_500_test)
r2_1000 = r2_score(y_test, predictions_1000_test)
r2_2000 = r2_score(y_test, predictions_2000_test)
r2_5000 = r2_score(y_test, predictions_5000_test)

# Create performance comparison
performance_data = {
    'Trees': [1, 5, 25, 100, 500, 1000, 2000, 5000],
    'RMSE_Test': [rmse_1_test, rmse_5_test, rmse_25_test, rmse_100_test, rmse_500_test, rmse_1000_test, rmse_2000_test, rmse_5000_test],
    'RMSE_Train': [rmse_1_train, rmse_5_train, rmse_25_train, rmse_100_train, rmse_500_train, rmse_1000_train, rmse_2000_train, rmse_5000_train],
    'R_squared': [r2_1, r2_5, r2_25, r2_100, r2_500, r2_1000, r2_2000, r2_5000]
}

performance_df = pd.DataFrame(performance_data)
print(performance_df)
```

:::

## Student Analysis Section: The Power of More Trees {#student-analysis-section}

**Your Task:** Create visualizations and analysis to demonstrate the power of ensemble learning. You'll need to create three key components:

### 1. The Power of More Trees Visualization

**Create a visualization showing:**
- RMSE vs Number of Trees (both training and test data)
- R-squared vs Number of Trees
- Do not `echo` the code that creates the visualization

```{python}
#| label: power-of-trees-visualization
#| echo: false
#| fig-width: 7
#| fig-height: 5

import matplotlib.pyplot as plt
import numpy as np

# Create figure with two subplots
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))

# Plot 1: RMSE vs Number of Trees
trees = performance_df['Trees']
rmse_test = performance_df['RMSE_Test']
rmse_train = performance_df['RMSE_Train']

ax1.plot(trees, rmse_test, 'o-', label='Test RMSE', linewidth=2, markersize=6)
ax1.plot(trees, rmse_train, 's-', label='Training RMSE', linewidth=2, markersize=6)
ax1.set_xlabel('Number of Trees')
ax1.set_ylabel('RMSE')
ax1.set_title('RMSE vs Number of Trees')
ax1.set_xscale('log')
ax1.legend()
ax1.grid(True, alpha=0.3)

# Plot 2: R-squared vs Number of Trees
r2_scores = performance_df['R_squared']
ax2.plot(trees, r2_scores, 'o-', color='green', linewidth=2, markersize=6)
ax2.set_xlabel('Number of Trees')
ax2.set_ylabel('R-squared')
ax2.set_title('R-squared vs Number of Trees')
ax2.set_xscale('log')
ax2.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

**Analysis: The Power of Ensemble Learning and Diminishing Returns**

The visualization reveals several key insights about how random forest performance improves with more trees, demonstrating fundamental principles of ensemble learning:

**Dramatic Early Improvements:**
The most significant performance gains occur in the first 25-100 trees. This dramatic improvement reflects the core principle of ensemble learning: combining multiple weak learners (individual decision trees) creates a strong learner. Each additional tree in this early range contributes substantially to reducing prediction variance and improving generalization.

**The Bootstrap Aggregation Effect:**
Random forests use bootstrap sampling (bagging) to create diversity among trees. With only 1-5 trees, the ensemble lacks sufficient diversity to capture the full complexity of the data. As we add more trees (25-100), we see the "wisdom of crowds" effect - each tree votes on the prediction, and the ensemble's decision becomes more robust and accurate.

**Diminishing Returns Beyond 100 Trees:**
After approximately 100 trees, the performance improvements become increasingly marginal. This demonstrates the principle of diminishing returns in ensemble learning. The additional trees provide less new information because:
- The bootstrap samples become more similar as we add trees
- The random feature selection has already explored most meaningful feature combinations
- The ensemble has reached a point where additional trees primarily reduce variance rather than bias

**Training vs Test Performance Gap:**
The gap between training and test RMSE remains relatively stable across different tree counts, indicating that random forests maintain good generalization even with many trees. This contrasts with individual decision trees, which would show increasing overfitting with complexity.

**Practical Implications:**
The analysis suggests that 100-500 trees often provide the optimal balance between performance and computational efficiency. Beyond 1000 trees, the computational cost typically outweighs the marginal performance gains, making larger ensembles impractical for most real-world applications.

This demonstrates why random forests are so powerful: they transform weak individual learners into a strong ensemble through the principles of diversity, voting, and bootstrap aggregation, with the most dramatic improvements occurring in the early stages of ensemble building.



### 2. Overfitting Visualization and Analysis

**Your Task:** Compare decision trees vs random forests in terms of overfitting.

**Create one visualization with two side-by-side plots showing:**
- Decision trees: How performance changes with tree complexity (max depth)
- Random forests: How performance changes with number of trees

```{python}
#| label: overfitting-comparison
#| echo: false
#| fig-width: 7
#| fig-height: 5

from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error
import numpy as np

# Test different max depths for decision trees
max_depths = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 12, 15, 20, 25, 30]
dt_train_rmse = []
dt_test_rmse = []

for depth in max_depths:
    dt = DecisionTreeRegressor(max_depth=depth, random_state=42)
    dt.fit(X_train, y_train)
    
    train_pred = dt.predict(X_train)
    test_pred = dt.predict(X_test)
    
    dt_train_rmse.append(np.sqrt(mean_squared_error(y_train, train_pred)))
    dt_test_rmse.append(np.sqrt(mean_squared_error(y_test, test_pred)))

# Test different numbers of trees for random forests
n_trees = [1, 5, 10, 25, 50, 100, 200, 500, 1000]
rf_train_rmse = []
rf_test_rmse = []

for n_tree in n_trees:
    rf = RandomForestRegressor(n_estimators=n_tree, random_state=42)
    rf.fit(X_train, y_train)
    
    train_pred = rf.predict(X_train)
    test_pred = rf.predict(X_test)
    
    rf_train_rmse.append(np.sqrt(mean_squared_error(y_train, train_pred)))
    rf_test_rmse.append(np.sqrt(mean_squared_error(y_test, test_pred)))

# Create the visualization
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))

# Plot 1: Decision Trees - Overfitting
ax1.plot(max_depths, dt_train_rmse, 'o-', label='Training RMSE', linewidth=2, markersize=6)
ax1.plot(max_depths, dt_test_rmse, 's-', label='Test RMSE', linewidth=2, markersize=6)
ax1.set_xlabel('Max Depth')
ax1.set_ylabel('RMSE')
ax1.set_title('Decision Trees: Overfitting with Complexity')
ax1.legend()
ax1.grid(True, alpha=0.3)

# Plot 2: Random Forests - No Overfitting
ax2.plot(n_trees, rf_train_rmse, 'o-', label='Training RMSE', linewidth=2, markersize=6)
ax2.plot(n_trees, rf_test_rmse, 's-', label='Test RMSE', linewidth=2, markersize=6)
ax2.set_xlabel('Number of Trees')
ax2.set_ylabel('RMSE')
ax2.set_title('Random Forests: No Overfitting with More Trees')
ax2.set_xscale('log')
ax2.legend()
ax2.grid(True, alpha=0.3)

# Set same y-axis limits for comparison
y_min = min(min(dt_train_rmse), min(dt_test_rmse), min(rf_train_rmse), min(rf_test_rmse))
y_max = max(max(dt_train_rmse), max(dt_test_rmse), max(rf_train_rmse), max(rf_test_rmse))
ax1.set_ylim(y_min, y_max)
ax2.set_ylim(y_min, y_max)

plt.tight_layout()
plt.show()
```

**Analysis: Why Random Forests Don't Overfit Like Individual Trees**

The side-by-side comparison reveals fundamental differences in how decision trees and random forests handle complexity:

**Decision Trees: The Overfitting Problem**
Individual decision trees show classic overfitting behavior as complexity increases. The training RMSE continues to decrease (approaching zero) while test RMSE increases after a certain point. This occurs because:
- **High Variance**: Individual trees can memorize training data with deep splits
- **No Regularization**: Without constraints, trees can create overly specific rules
- **Single Path Decisions**: Each prediction follows one path, making the model brittle to noise

**Random Forests: The Overfitting Solution**
Random forests maintain stable performance even with many trees. The training and test RMSE curves remain close together, demonstrating excellent generalization. This occurs through three key mechanisms:

1. **Bootstrap Sampling (Bagging)**: Each tree trains on a different subset of data, preventing memorization of the entire training set
2. **Random Feature Selection**: Each split considers only a random subset of features, reducing correlation between trees
3. **Averaging Predictions**: The final prediction averages across many diverse trees, smoothing out individual tree errors

**The Ensemble Effect**
The gap between training and test performance remains stable in random forests because:
- Individual trees are diverse enough to capture different patterns
- No single tree can overfit to the entire training set
- The averaging process reduces variance while maintaining bias

**Practical Implications**
This analysis demonstrates why random forests are preferred over individual decision trees for most real-world applications. Random forests provide the interpretability benefits of tree-based models while avoiding the overfitting pitfalls that plague individual trees.


### 3. Linear Regression vs Random Forest Comparison

**Your Task:** Compare random forests to linear regression baseline.

```{python}
#| label: linear-regression-comparison
#| echo: false
#| fig-width: 7
#| fig-height: 5

from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score
import pandas as pd

# Create and fit linear regression model
lr = LinearRegression()
lr.fit(X_train, y_train)
lr_pred_test = lr.predict(X_test)
lr_pred_train = lr.predict(X_train)
lr_rmse_test = np.sqrt(mean_squared_error(y_test, lr_pred_test))
lr_rmse_train = np.sqrt(mean_squared_error(y_train, lr_pred_train))
lr_r2_test = r2_score(y_test, lr_pred_test)

# Create and fit random forest models
rf_1 = RandomForestRegressor(n_estimators=1, random_state=42)
rf_100 = RandomForestRegressor(n_estimators=100, random_state=42)
rf_1000 = RandomForestRegressor(n_estimators=1000, random_state=42)

rf_1.fit(X_train, y_train)
rf_100.fit(X_train, y_train)
rf_1000.fit(X_train, y_train)

# Get predictions and metrics
rf_1_pred_test = rf_1.predict(X_test)
rf_1_pred_train = rf_1.predict(X_train)
rf_1_rmse_test = np.sqrt(mean_squared_error(y_test, rf_1_pred_test))
rf_1_rmse_train = np.sqrt(mean_squared_error(y_train, rf_1_pred_train))
rf_1_r2_test = r2_score(y_test, rf_1_pred_test)

rf_100_pred_test = rf_100.predict(X_test)
rf_100_pred_train = rf_100.predict(X_train)
rf_100_rmse_test = np.sqrt(mean_squared_error(y_test, rf_100_pred_test))
rf_100_rmse_train = np.sqrt(mean_squared_error(y_train, rf_100_pred_train))
rf_100_r2_test = r2_score(y_test, rf_100_pred_test)

rf_1000_pred_test = rf_1000.predict(X_test)
rf_1000_pred_train = rf_1000.predict(X_train)
rf_1000_rmse_test = np.sqrt(mean_squared_error(y_test, rf_1000_pred_test))
rf_1000_rmse_train = np.sqrt(mean_squared_error(y_train, rf_1000_pred_train))
rf_1000_r2_test = r2_score(y_test, rf_1000_pred_test)

# Create comparison table
comparison_data = {
    'Model': ['Linear Regression', 'Random Forest (1 tree)', 'Random Forest (100 trees)', 'Random Forest (1000 trees)'],
    'Test RMSE': [lr_rmse_test, rf_1_rmse_test, rf_100_rmse_test, rf_1000_rmse_test],
    'Training RMSE': [lr_rmse_train, rf_1_rmse_train, rf_100_rmse_train, rf_1000_rmse_train],
    'Test RÂ²': [lr_r2_test, rf_1_r2_test, rf_100_r2_test, rf_1000_r2_test],
    'Improvement over Linear Regression (%)': [
        0,
        round((lr_rmse_test - rf_1_rmse_test) / lr_rmse_test * 100, 1),
        round((lr_rmse_test - rf_100_rmse_test) / lr_rmse_test * 100, 1),
        round((lr_rmse_test - rf_1000_rmse_test) / lr_rmse_test * 100, 1)
    ]
}

comparison_df = pd.DataFrame(comparison_data)
print("Model Performance Comparison")
print("=" * 50)
print(comparison_df.to_string(index=False))
```

**Analysis: When Random Forests Are Worth the Complexity**

The comparison reveals important insights about the trade-offs between simplicity and performance:

**Performance Improvements:**
- **1 Tree to 100 Trees**: The improvement from 1 tree to 100 trees typically shows a dramatic reduction in RMSE, demonstrating the power of ensemble learning
- **Linear Regression to 100-Tree Random Forest**: The improvement from linear regression to a 100-tree random forest often shows similar magnitude improvements, suggesting that ensemble learning can provide substantial benefits over linear models
- **100 to 1000 Trees**: The improvement from 100 to 1000 trees is typically marginal, confirming the principle of diminishing returns

**When Random Forests Are Worth the Complexity:**
Random forests are worth the added complexity when:
- **Non-linear relationships exist** in the data that linear regression cannot capture
- **Feature interactions** are important for prediction accuracy
- **Performance gains** justify the computational cost and reduced interpretability
- **Robust predictions** are needed across diverse data patterns

**Real-World Application Scenarios:**

**Choose Linear Regression When:**
- **Regulatory compliance** requires model interpretability (finance, healthcare)
- **Business stakeholders** need to understand feature importance through coefficients
- **Fast inference** is critical (real-time systems, mobile applications)
- **Data relationships** are approximately linear (sales vs advertising spend, simple pricing models)
- **Computational resources** are limited (edge devices, embedded systems)

**Choose Random Forests When:**
- **Prediction accuracy** is more important than interpretability (recommendation systems, fraud detection)
- **Complex feature interactions** exist (customer behavior, market dynamics)
- **Non-linear patterns** are present (image recognition, natural language processing)
- **Robust performance** across diverse data is needed (multi-market applications)
- **Feature engineering** is challenging (automated feature selection benefits)

**Practical Decision Framework:**
1. **Start with linear regression** as a baseline - it's fast, interpretable, and often surprisingly effective
2. **Upgrade to random forests** if linear regression performance is insufficient and the business case supports the added complexity
3. **Consider the audience** - technical teams can handle complex models, but business stakeholders may prefer interpretable models
4. **Evaluate the trade-offs** - 10% performance improvement may not justify 10x computational cost in some applications

**Key Insight**: The comparison demonstrates that random forests often provide substantial performance improvements over linear regression, but the decision to use them depends on whether the performance gains justify the added complexity and reduced interpretability for your specific use case. In practice, many successful data science projects start with linear regression and only move to more complex models when the business value is clear.

**Your analysis should address:**
- The improvement in RMSE when going from 1 tree to 100 trees
- Whether switching from linear regression to 100-tree random forest shows similar improvement
- When random forests are worth the added complexity vs linear regression
- The trade-offs between interpretability and performance


Create a clear table comparing:

- Linear Regression
- Random Forest (1 tree)
- Random Forest (100 trees) 
- Random Forest (1000 trees)

Include percentage improvements over linear regression for each random forest model.
:::

**This is an investigative report, not a coding exercise.** You're analyzing random forest models and reporting your findings like a professional analyst would. Think of this as a brief you'd write for a client or manager about the power of ensemble learning and when to use random forests vs simpler approaches.

**What makes a great report:**

- **Clear narrative:** Tell the story of what you discovered about ensemble learning
- **Insightful analysis:** Focus on the most interesting findings about random forest performance
- **Professional presentation:** Clean, readable, and engaging
- **Concise conclusions:** No AI babble or unnecessary technical jargon
- **Human insights:** Your interpretation of what the performance improvements actually mean
- **Practical implications:** When random forests are worth the added complexity

**What we're looking for:** A compelling 2-3 minute read that demonstrates both the power of ensemble learning and the importance of choosing the right tool for the job.
:::